# -*- coding: utf-8 -*-
"""Pic2Product.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-9pbZOTB8AlAVTZaq3YYW-Y7skKvCFJ9
"""

!pip install git+https://github.com/serpapi/google-search-results-python.git

from serpapi import GoogleSearch

print("‚úÖ SerpAPI is now working!")

!pip install beautifulsoup4

!pip install transformers google-search-results beautifulsoup4 requests

from google.colab import files
uploaded = files.upload()

from PIL import Image
import os

uploaded_filename = list(uploaded.keys())[0]
image = Image.open(uploaded_filename).convert("RGB")

from transformers import BlipProcessor, BlipForConditionalGeneration

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

inputs = processor(images=image, return_tensors="pt")
output = model.generate(**inputs)
caption = processor.decode(output[0], skip_special_tokens=True)

print(f"\n Image caption extracted: \"{caption}\"")

from serpapi import GoogleSearch
import requests
from io import BytesIO
import IPython.display as display

params = {
    "engine": "google",
    "q": caption,  # Auto-generated caption
    "tbm": "isch",  # image search
    "api_key": "f5690439d2ec477a1329ff0fc525bf9fbfd7ee9d150b2f13bab3facaa7a1e838"
}

search = GoogleSearch(params)
results = search.get_dict()
image_results = results.get("images_results", [])

print("\n Visually Similar Images Found:")
for result in image_results[:5]:
    img_url = result.get("original")
    if img_url:
        response = requests.get(img_url)
        similar_img = Image.open(BytesIO(response.content))
        display.display(similar_img)

from google.colab import files
import csv
import re
import requests
from bs4 import BeautifulSoup
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration
from serpapi import GoogleSearch

uploaded = files.upload()
uploaded_filename = list(uploaded.keys())[0]
image = Image.open(uploaded_filename).convert("RGB")

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
inputs = processor(images=image, return_tensors="pt")
output = model.generate(**inputs)
caption = processor.decode(output[0], skip_special_tokens=True)
print(f"\n Image caption extracted: \"{caption}\"")

params = {
    "engine": "google",
    "q": caption,
    "tbm": "isch",
    "api_key": "f5690439d2ec477a1329ff0fc525bf9fbfd7ee9d150b2f13bab3facaa7a1e838"
}

search = GoogleSearch(params)
results = search.get_dict()
image_results = results.get("images_results", [])

j = int(input("Enter how many website links you want to fetch: "))
data_to_save = []

print("\n Website links and prices:")
for i, result in enumerate(image_results[:j], 1):
    title = result.get("title", "No Title")
    link = result.get("link")
    print(f"\n{i}. {title}\n{link}")

    price = "Not Found"
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(link, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        text = soup.get_text()

        price_match = re.search(r'(\$|‚Çπ|Rs\.?)\s?\d{1,3}(?:[,\d]{3})*(?:\.\d{1,2})?', text)
        if price_match:
            price = price_match.group(0)
    except Exception as e:
        print(f"    Error fetching price: {e}")

    print(f"    Price: {price}")
    data_to_save.append([title, link, price])

csv_filename = "image_search_with_prices.csv"
with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(["Title", "Website Link", "Price"])
    writer.writerows(data_to_save)

print(f"\n CSV file saved as '{csv_filename}'")
files.download(csv_filename)

!pip install tldextract

from google.colab import files
import csv, re, requests, tldextract
from bs4 import BeautifulSoup
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration
from serpapi import GoogleSearch

uploaded = files.upload()
filename = list(uploaded.keys())[0]
image = Image.open(filename).convert("RGB")

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
inputs = processor(images=image, return_tensors="pt")
caption = processor.decode(model.generate(**inputs)[0], skip_special_tokens=True)
print(f"\nüß† Caption: \"{caption}\"\n")

# --- Step 2: Google Image Search ---
serpapi_api_key = "YOUR_SERPAPI_KEY"
params = {
    "engine": "google",
    "q": caption,
    "tbm": "isch",
    "api_key": "f5690439d2ec477a1329ff0fc525bf9fbfd7ee9d150b2f13bab3facaa7a1e838"
}
search = GoogleSearch(params)
image_results = search.get_dict().get("images_results", [])

# --- Step 3: Desired Product Count ---
target_count = int(input("Enter number of product links to fetch: "))
exclude_domains = {"amazon.com", "flipkart.com", "ebay.com", "ebuy.com","walmart.com"}
headers = {'User-Agent': 'Mozilla/5.0'}
data = []
fetched = 0
i = 0

# --- Step 4: Scrape Until Target is Reached ---
while fetched < target_count and i < len(image_results):
    result = image_results[i]
    link = result.get("link", "")
    title = result.get("title", "No Title")
    domain = tldextract.extract(link).registered_domain.lower()
    i += 1

    if domain in exclude_domains:
        print(f"‚û°Ô∏è Skipping excluded site: {domain}")
        continue

    print(f"\n{fetched+1}. {title}\n{link}")

    price = "Not Found"
    rating = "N/A"
    reviews = "N/A"
    stock = "Unknown"
    delivery = "Unknown"
    description = ""

    try:
        resp = requests.get(link, headers=headers, timeout=10)
        soup = BeautifulSoup(resp.text, "html.parser")
        text = soup.get_text(separator=" ").strip()

        pm = re.search(r'(\$|‚Çπ|Rs\.?)\s?\d{1,3}(?:[,\d]{3})*(?:\.\d{1,2})?', text)
        if pm: price = pm.group(0)

        rm = re.search(r'(\d\.\d)\s?out of\s?5', text)
        if rm: rating = rm.group(1) + "/5"

        revm = re.search(r'([\d,]+)\s+(?:reviews|ratings)', text)
        if revm: reviews = revm.group(1)

        if re.search(r'\b(in stock|available now)\b', text, re.IGNORECASE):
            stock = "In Stock"
        elif re.search(r'\b(out of stock|unavailable)\b', text, re.IGNORECASE):
            stock = "Out of Stock"

        dm = re.search(r'(deliver(?:y|ed) by [A-Za-z0-9,\s]+)', text, re.IGNORECASE)
        if not dm:
            dm = re.search(r'(ships? by [A-Za-z0-9,\s]+)', text, re.IGNORECASE)
        if dm: delivery = dm.group(1)

        meta = soup.find("meta", attrs={"name": "description"})
        description = meta["content"].strip() if meta and meta.get("content") else soup.find("p", "").get_text()[:300] if soup.find("p") else ""

    except Exception as e:
        print(f"‚ö†Ô∏è Error scraping {link}: {e}")

    print(f"  Price: {price} | Stock: {stock} | Delivery: {delivery}")
    data.append([title, link, price, rating, reviews, stock, delivery, domain, description])
    fetched += 1

# --- Step 5: Save to CSV ---
csv_name = "products_detailed.csv"
with open(csv_name, "w", newline="", encoding="utf-8") as f:
    writer = csv.writer(f)
    writer.writerow(["Title", "Link", "Price", "Rating", "Reviews", "Stock Status", "Delivery Info", "Retailer", "Description"])
    writer.writerows(data)

print(f"\n‚úÖ CSV saved as {csv_name}")
files.download(csv_name)

from google.colab import files
import csv, re, requests, time
from bs4 import BeautifulSoup
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration

# Upload the image
uploaded = files.upload()
uploaded_filename = list(uploaded.keys())[0]
image = Image.open(uploaded_filename).convert("RGB")

# Generate a caption using BLIP
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
inputs = processor(images=image, return_tensors="pt")
output = model.generate(**inputs)
caption = processor.decode(output[0], skip_special_tokens=True)
print(f"\nüñºÔ∏è Image caption extracted: \"{caption}\"")

# DuckDuckGo Web Search (not image search)
def duckduckgo_web_search(query, max_results=10):
    print(f"\nüîç Performing DuckDuckGo web search for: \"{query}\"")
    headers = {'User-Agent': 'Mozilla/5.0'}
    params = {'q': query}
    results = []

    try:
        res = requests.post("https://html.duckduckgo.com/html/", data=params, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        for a in soup.select('a.result__a'):
            title = a.text.strip()
            href = a['href']
            if href.startswith('/l/?kh='):
                match = re.search(r'u=(.*?)&', href)
                if match:
                    href = requests.utils.unquote(match.group(1))
            if not href.endswith(('.jpg', '.jpeg', '.png', '.gif')):
                results.append({'title': title, 'link': href})
            if len(results) >= max_results:
                break
    except Exception as e:
        print(f"‚ùå Error during web search: {e}")

    return results

# Filter unsafe or blocked domains
def is_valid_link(link):
    if not link or not link.startswith("http"):
        return False
    blacklist = [
        "pinterest", "facebook", "duckduckgo.com", "tumblr", "flickr",
        "amazon.", "flipkart.", "ebay.", "aliexpress.", "walmart.",
        "snapdeal.", "myntra.", "shopify.", "bestbuy.", "target.com", "ajio."
    ]
    return not any(domain in link.lower() for domain in blacklist)

# Run the web search
results = duckduckgo_web_search(caption, max_results=20)
j = int(input("üî¢ Enter how many valid product pages you want to fetch: "))

data_to_save = []
headers = {'User-Agent': 'Mozilla/5.0'}
count = 0

print("\nüõí Extracting product data from valid pages...")
for i, result in enumerate(results):
    if count >= j:
        break

    title = result.get("title", "No Title")
    link = result.get("link", "")
    if not is_valid_link(link):
        continue

    print(f"\n{count + 1}. {title}\nüîó {link}")
    time.sleep(1)  # Be respectful to servers

    price, rating, reviews, description = "Not Found", "N/A", "N/A", ""

    try:
        response = requests.get(link, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        text = soup.get_text()

        # Price detection
        price_match = re.search(r'(\$|‚Çπ|Rs\.?)\s?\d{1,3}(?:[,\d]{3})*(?:\.\d{1,2})?', text)
        if price_match:
            price = price_match.group(0)

        # Rating detection
        rating_match = re.search(r'(\d\.\d)\s?out of\s?5', text)
        if rating_match:
            rating = rating_match.group(1) + "/5"

        # Review count
        review_match = re.search(r'([\d,]+)\s+(?:reviews|ratings)', text)
        if review_match:
            reviews = review_match.group(1)

        # Description
        meta_desc = soup.find("meta", attrs={"name": "description"})
        if meta_desc and meta_desc.get("content"):
            description = meta_desc["content"].strip()
        else:
            first_p = soup.find("p")
            if first_p:
                description = first_p.get_text().strip()[:300]

    except Exception as e:
        print(f"   ‚ö†Ô∏è Error fetching product data: {e}")

    print(f"   üí∞ Price: {price}\n   ‚≠ê Rating: {rating}\n   üó£Ô∏è Reviews: {reviews}")
    data_to_save.append([title, link, price, rating, reviews, description])
    count += 1

# Save results to CSV
csv_filename = "image_search_products_duckduckgo_filtered.csv"
with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(["Title", "Website Link", "Price", "Rating", "Reviews Count", "Product Description"])
    writer.writerows(data_to_save)

print(f"\n‚úÖ CSV file saved as '{csv_filename}'")
files.download(csv_filename)

!pip install ultralytics

from google.colab import files
import torch, os, re, csv, requests, time, cv2
from PIL import Image
from ultralytics import YOLO
from bs4 import BeautifulSoup
from collections import defaultdict

# Step 1: Upload video
uploaded = files.upload()
video_path = list(uploaded.keys())[0]

# Step 2: Load YOLOv8 model
model = YOLO("yolov8n.pt")  # or yolov8s.pt for better accuracy

# Step 3: Extract frames and detect objects
print("\nüéØ Extracting objects from video...")
cap = cv2.VideoCapture(video_path)
fps = int(cap.get(cv2.CAP_PROP_FPS))
frame_interval = fps * 2  # every 2 seconds
frame_count = 0
object_images = defaultdict(list)
max_objects_per_class = 10

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    if frame_count % frame_interval == 0:
        results = model(frame)[0]
        for box in results.boxes:
            cls = int(box.cls)
            label = model.names[cls]
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            crop = frame[y1:y2, x1:x2]
            if len(object_images[label]) < max_objects_per_class:
                object_images[label].append(Image.fromarray(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)))
    frame_count += 1

cap.release()
print(f"‚úÖ Objects detected: {list(object_images.keys())}")

# Step 4: Web Search per object (DuckDuckGo)
def duckduckgo_search(query, max_results=10):
    headers = {'User-Agent': 'Mozilla/5.0'}
    params = {'q': query}
    results = []
    try:
        res = requests.post("https://html.duckduckgo.com/html/", data=params, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        for a in soup.select('a.result__a'):
            title = a.text.strip()
            href = a['href']
            if href.startswith('/l/?kh='):
                match = re.search(r'u=(.*?)&', href)
                if match:
                    href = requests.utils.unquote(match.group(1))
            if not href.endswith(('.jpg', '.jpeg', '.png', '.gif')):
                results.append({'title': title, 'link': href})
            if len(results) >= max_results:
                break
    except Exception as e:
        print(f"‚ùå Search error for '{query}': {e}")
    return results

def is_valid_link(link):
    if not link or not link.startswith("http"):
        return False
    whitelist = [
        "amazon.", "flipkart.", "ebay.", "aliexpress.", "walmart.",
        "snapdeal.", "myntra.", "shopify.", "bestbuy.", "target.com", "ajio."
    ]
    return any(domain in link.lower() for domain in whitelist)


# Step 5: Scrape details from product page
def extract_product_data(link):
    headers = {'User-Agent': 'Mozilla/5.0'}
    price, rating, reviews, description = "Not Found", "N/A", "N/A", ""
    try:
        response = requests.get(link, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        text = soup.get_text()

        price_match = re.search(r'(\$|‚Çπ|Rs\.?)\s?\d{1,3}(?:[,\d]{3})*(?:\.\d{1,2})?', text)
        if price_match:
            price = price_match.group(0)

        rating_match = re.search(r'(\d\.\d)\s?out of\s?5', text)
        if rating_match:
            rating = rating_match.group(1) + "/5"

        review_match = re.search(r'([\d,]+)\s+(?:reviews|ratings)', text)
        if review_match:
            reviews = review_match.group(1)

        meta_desc = soup.find("meta", attrs={"name": "description"})
        if meta_desc and meta_desc.get("content"):
            description = meta_desc["content"].strip()
        else:
            first_p = soup.find("p")
            if first_p:
                description = first_p.get_text().strip()[:300]

    except Exception as e:
        print(f"‚ö†Ô∏è Scrape error for {link}: {e}")
    return price, rating, reviews, description

# Step 6: Search and Scrape for each object type
data_to_save = []

for obj_name, images in object_images.items():
    print(f"\nüîç Searching for: {obj_name}")
    search_results = duckduckgo_search(f"buy {obj_name} online", max_results=30)
    count = 0
    for result in search_results:
        if is_valid_link(result["link"]):
            print(f"\nüõí {obj_name} ‚Üí {result['title']}")
            print(f"üîó {result['link']}")
            price, rating, reviews, desc = extract_product_data(result['link'])
            print(f"üí∞ Price: {price} | ‚≠ê Rating: {rating} | üó£Ô∏è Reviews: {reviews}")
            data_to_save.append([obj_name, result['title'], result['link'], price, rating, reviews, desc])
            count += 1
        if count >= 10:
            break

# Step 7: Save CSV
csv_filename = "video_objects_scraped.csv"
with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(["Object Type", "Title", "Website Link", "Price", "Rating", "Reviews Count", "Description"])
    writer.writerows(data_to_save)

print(f"\n‚úÖ Saved all product details to '{csv_filename}'")
files.download(csv_filename)